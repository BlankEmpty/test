{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import losses\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # [40, 7500, 4, 1]\n",
    "    import os\n",
    "    files = os.listdir('train')\n",
    "    all_list = []\n",
    "    label = []\n",
    "    for file in files:\n",
    "        train = pd.read_excel('./train/' + file, encoding='utf8', header=None)\n",
    "        # 加工品質量測結果:0.306\n",
    "        label.append(train.get_values()[-1][0].split(':')[1])        \n",
    "        \n",
    "        train = np.array(train.drop(7500, axis=0))\n",
    "        all_list.append(train)\n",
    "        \n",
    "    return np.array(all_list), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_data(train, label, pastDay=7500, futureDay=1):\n",
    "    assert train.shape[0] == label.shape[0]\n",
    "    \n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0] - pastDay - futureDay):\n",
    "        X_train.append(train[i: i + pastDay])\n",
    "        Y_train.append(label[i + pastDay: i + pastDay + futureDay])\n",
    "    \n",
    "    return np.array(X_train), np.array(Y_train)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(X, Y):\n",
    "    np.random.seed(1)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split some training data to validation data\n",
    "def splitData(X, Y, rate):\n",
    "    \n",
    "    X_train = X[int(X.shape[0] * rate): ]\n",
    "    Y_train = Y[int(Y.shape[0] * rate): ]\n",
    "    X_val = X[:int(X.shape[0] * rate)]\n",
    "    Y_val = Y[:int(Y.shape[0] * rate)]\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_LSTM_model(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(7500, 4), return_sequences=False))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=losses.mean_squared_error, optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_94 (Conv1D)           (None, 7497, 32)          544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 7497, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 1874, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 1871, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 1871, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 467, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 464, 128)          32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 464, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 116, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 113, 256)          131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 113, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 28, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 25, 512)           524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 25, 512)           2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,849,569\n",
      "Trainable params: 3,847,585\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 28 samples, validate on 12 samples\n",
      "Epoch 1/1000\n",
      "28/28 [==============================] - 5s 179ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.6670 - acc: 0.0000e+0 - 0s 10ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 37/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 21ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 6.0222 - acc: 0.0000e+00 - val_loss: 7.8088 - val_acc: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 5.5702 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5e966420436f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m train_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, \n\u001b[0;32m     58\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m           callbacks=[modelcheckpoint])\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   2554\u001b[0m         \"\"\"\n\u001b[0;32m   2555\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2556\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2558\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mmodel_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'optimizer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m   2862\u001b[0m             param_dset = g.create_dataset(name, val.shape,\n\u001b[0;32m   2863\u001b[0m                                           dtype=val.dtype)\n\u001b[1;32m-> 2864\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2865\u001b[0m                 \u001b[1;31m# scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2866\u001b[0m                 \u001b[0mparam_dset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import Embedding, Conv1D, SpatialDropout1D, GlobalMaxPool1D, MaxPooling1D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filter_length = 4\n",
    "nb_features = 7500\n",
    "\n",
    "output_dir = 'model_output/conv'\n",
    "batch_size = 5\n",
    "epochs = 1000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, filter_length, activation='relu', input_shape=(7500, 4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4,strides=2, padding='VALID'))\n",
    "\n",
    "model.add(Conv1D(64, filter_length, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4,strides=2, padding='VALID'))\n",
    "\n",
    "model.add(Conv1D(128, filter_length, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4,strides=2, padding='VALID'))\n",
    "\n",
    "model.add(Conv1D(256, filter_length, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4,strides=2, padding='VALID'))\n",
    "\n",
    "model.add(Conv1D(512, filter_length, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4, strides=2, padding='VALID'))\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+'/weights.{epoch:02d}.hdf5')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "train, label = read_data()\n",
    "X_train, Y_train, X_val, Y_val = splitData(train, label, 0.3)\n",
    "\n",
    "train_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_split=.20, validation_data=(X_val, Y_val),\n",
    "          callbacks=[modelcheckpoint])\n",
    "\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('Train History')\n",
    "# plt.ylabel(train)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 7500, 4)\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 611\n",
      "Trainable params: 611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 28 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.4215 - val_loss: 0.2561\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.3913 - val_loss: 0.2298\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 0.3573 - val_loss: 0.2004\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.3190 - val_loss: 0.1675\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.2744 - val_loss: 0.1304\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.2224 - val_loss: 0.0886\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.1647 - val_loss: 0.0453\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0959 - val_loss: 0.0182\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0468 - val_loss: 0.0598\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0604 - val_loss: 0.0631\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0467 - val_loss: 0.0250\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0497 - val_loss: 0.0196\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0494 - val_loss: 0.0224\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0458 - val_loss: 0.0287\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0441 - val_loss: 0.0320\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0371\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0454 - val_loss: 0.0378\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0460 - val_loss: 0.0297\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0449 - val_loss: 0.0302\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0454 - val_loss: 0.0276\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0448 - val_loss: 0.0348\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0451 - val_loss: 0.0316\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0307\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0447 - val_loss: 0.0311\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "train, label = read_data()\n",
    "\n",
    "X_train, Y_train = build_train_data(train, label)\n",
    "\n",
    "# # shuffle the data\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = splitData(train, label, 0.3)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "model = build_LSTM_model(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "train_history = model.fit(X_train, Y_train, epochs=100, batch_size=5, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# One to One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 7500, 4)\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 611\n",
      "Trainable params: 611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 28 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.4192 - val_loss: 0.2523\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.3855 - val_loss: 0.2217\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.3451 - val_loss: 0.1883\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.3027 - val_loss: 0.1513\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.2523 - val_loss: 0.1108\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.1956 - val_loss: 0.0678\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.1271 - val_loss: 0.0288\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0631 - val_loss: 0.0252\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0594 - val_loss: 0.0774\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0540 - val_loss: 0.0413\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0470 - val_loss: 0.0230\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0471 - val_loss: 0.0217\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0468 - val_loss: 0.0232\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0463 - val_loss: 0.0305\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0312\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0447 - val_loss: 0.0336\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0452 - val_loss: 0.0326\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0446 - val_loss: 0.0326\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0447 - val_loss: 0.0290\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0449 - val_loss: 0.0301\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0451 - val_loss: 0.0266\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0447 - val_loss: 0.0276\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0455 - val_loss: 0.0328\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0448 - val_loss: 0.0327\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0442 - val_loss: 0.0283\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0447 - val_loss: 0.0268\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0456 - val_loss: 0.0310\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0290\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0314\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0319\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0444 - val_loss: 0.0325\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0458 - val_loss: 0.0360\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0447 - val_loss: 0.0320\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0275\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0272\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "train, label = read_data()\n",
    "\n",
    "# X_train, Y_train = build_train_data(train, label, 5, 1)\n",
    "\n",
    "# # shuffle the data\n",
    "# X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = splitData(train, label, 0.3)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "model = build_LSTM_model(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "train_history = model.fit(X_train, Y_train, epochs=100, batch_size=5, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njTEmwVjiN8aYBPP/AdJl9hck8unPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a6f2dc8048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('Train History')\n",
    "# plt.ylabel(train)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 30, 7500, 32)      1184      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 7498, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 3749, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 14, 3749, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 3749, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 12, 3747, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 1873, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 6, 1873, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 6, 1873, 64)       36928     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 1871, 64)       36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 935, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 2, 935, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 119680)            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               61276672  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 61,421,514\n",
      "Trainable params: 61,421,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "input_shape = (30, 7500, 4)\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "     \n",
    "    return model\n",
    "\n",
    "model = createModel()\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
