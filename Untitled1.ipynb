{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import losses\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # [40, 7500, 4, 1]\n",
    "    import os\n",
    "    files = os.listdir('train')\n",
    "    all_list = []\n",
    "    label = []\n",
    "    for file in files:\n",
    "        train = pd.read_excel('./train/' + file, encoding='utf8', header=None)\n",
    "        # 加工品質量測結果:0.306\n",
    "        label.append(train.get_values()[-1][0].split(':')[1])        \n",
    "        \n",
    "        train = np.array(train.drop(7500, axis=0))\n",
    "        all_list.append(train)\n",
    "        \n",
    "    return np.array(all_list), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_data(train, label, pastDay=7500, futureDay=1):\n",
    "    assert train.shape[0] == label.shape[0]\n",
    "    \n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0] - pastDay - futureDay):\n",
    "        X_train.append(train[i: i + pastDay])\n",
    "        Y_train.append(label[i + pastDay: i + pastDay + futureDay])\n",
    "    \n",
    "    return np.array(X_train), np.array(Y_train)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(X, Y):\n",
    "    np.random.seed(1)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split some training data to validation data\n",
    "def splitData(X, Y, rate):\n",
    "    \n",
    "    X_train = X[int(X.shape[0] * rate): ]\n",
    "    Y_train = Y[int(Y.shape[0] * rate): ]\n",
    "    X_val = X[:int(X.shape[0] * rate)]\n",
    "    Y_val = Y[:int(Y.shape[0] * rate)]\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_LSTM_model(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(7500, 4), return_sequences=False))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=losses.mean_squared_error, optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional 1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_10 (TimeDis (None, 1, 4, 7500, 32)    320       \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 1, 4, 7500, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 1, 4, 7500, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 1, 4, 7500, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 1, 1, 1875, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 1, 1, 1875, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 1, 60000)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 1, 512)            30720512  \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 1, 35)             17955     \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 1, 20)             4480      \n",
      "_________________________________________________________________\n",
      "time_distr_dense_one (TimeDi (None, 1, 1)              21        \n",
      "_________________________________________________________________\n",
      "global_avg (GlobalAveragePoo (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 30,752,536\n",
      "Trainable params: 30,752,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 28 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 5s 196ms/step - loss: 1.9545 - acc: 0.0000e+00 - val_loss: 0.7410 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6750 - acc: 0.0000e+00 - val_loss: 0.6930 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 2s 84ms/step - loss: 0.6841 - acc: 0.0000e+00 - val_loss: 0.7250 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6804 - acc: 0.0000e+00 - val_loss: 0.7577 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6791 - acc: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 3s 108ms/step - loss: 0.6751 - acc: 0.0000e+00 - val_loss: 0.7360 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6794 - acc: 0.0000e+00 - val_loss: 0.7276 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6636 - acc: 0.0000e+00 - val_loss: 0.6985 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 2s 88ms/step - loss: 0.6675 - acc: 0.0000e+00 - val_loss: 0.7236 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6811 - acc: 0.0000e+00 - val_loss: 0.7240 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 3s 97ms/step - loss: 0.6650 - acc: 0.0000e+00 - val_loss: 0.7016 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.6685 - acc: 0.0000e+00 - val_loss: 0.7292 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 2s 89ms/step - loss: 0.6725 - acc: 0.0000e+00 - val_loss: 0.7274 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6721 - acc: 0.0000e+00 - val_loss: 0.7111 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 3s 97ms/step - loss: 0.6820 - acc: 0.0000e+00 - val_loss: 0.7035 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6636 - acc: 0.0000e+00 - val_loss: 0.7560 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.6715 - acc: 0.0000e+00 - val_loss: 0.7195 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6727 - acc: 0.0000e+00 - val_loss: 0.7188 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 2s 88ms/step - loss: 0.6643 - acc: 0.0000e+00 - val_loss: 0.7515 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.6804 - acc: 0.0000e+00 - val_loss: 0.7072 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6807 - acc: 0.0000e+00 - val_loss: 0.7369 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6839 - acc: 0.0000e+00 - val_loss: 0.6992 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 2s 84ms/step - loss: 0.6648 - acc: 0.0000e+00 - val_loss: 0.7630 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.6671 - acc: 0.0000e+00 - val_loss: 0.7007 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 3s 93ms/step - loss: 0.7038 - acc: 0.0000e+00 - val_loss: 0.7092 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6623 - acc: 0.0000e+00 - val_loss: 0.7613 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 3s 109ms/step - loss: 0.6913 - acc: 0.0000e+00 - val_loss: 0.7156 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6954 - acc: 0.0000e+00 - val_loss: 0.7115 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6650 - acc: 0.0000e+00 - val_loss: 0.6995 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.6664 - acc: 0.0000e+00 - val_loss: 0.7365 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6668 - acc: 0.0000e+00 - val_loss: 0.7130 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 2s 82ms/step - loss: 0.6714 - acc: 0.0000e+00 - val_loss: 0.7111 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 3s 94ms/step - loss: 0.6778 - acc: 0.0000e+00 - val_loss: 0.7137 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6681 - acc: 0.0000e+00 - val_loss: 0.7417 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 2s 84ms/step - loss: 0.6814 - acc: 0.0000e+00 - val_loss: 0.7092 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 3s 103ms/step - loss: 0.7022 - acc: 0.0000e+00 - val_loss: 0.7045 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.7071 - acc: 0.0000e+00 - val_loss: 0.7025 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.6990 - acc: 0.0000e+00 - val_loss: 0.7373 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.7806 - acc: 0.0000e+00 - val_loss: 0.6953 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 2s 88ms/step - loss: 0.7134 - acc: 0.0000e+00 - val_loss: 0.7605 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.7091 - acc: 0.0000e+00 - val_loss: 0.6949 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 3s 97ms/step - loss: 0.6609 - acc: 0.0000e+00 - val_loss: 0.8659 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6898 - acc: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.7029 - acc: 0.0000e+00 - val_loss: 0.7394 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 3s 102ms/step - loss: 0.6898 - acc: 0.0000e+00 - val_loss: 0.7248 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6642 - acc: 0.0000e+00 - val_loss: 0.7190 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 2s 84ms/step - loss: 0.6707 - acc: 0.0000e+00 - val_loss: 0.7044 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6767 - acc: 0.0000e+00 - val_loss: 0.7033 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6637 - acc: 0.0000e+00 - val_loss: 0.7579 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 3s 89ms/step - loss: 0.6794 - acc: 0.0000e+00 - val_loss: 0.7099 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 3s 92ms/step - loss: 0.6676 - acc: 0.0000e+00 - val_loss: 0.7135 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6674 - acc: 0.0000e+00 - val_loss: 0.7094 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6637 - acc: 0.0000e+00 - val_loss: 0.7412 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.6780 - acc: 0.0000e+00 - val_loss: 0.7100 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.6648 - acc: 0.0000e+00 - val_loss: 0.7490 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6729 - acc: 0.0000e+00 - val_loss: 0.7136 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - 3s 101ms/step - loss: 0.7099 - acc: 0.0000e+00 - val_loss: 0.7104 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.6845 - acc: 0.0000e+00 - val_loss: 0.7493 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 2s 88ms/step - loss: 0.7111 - acc: 0.0000e+00 - val_loss: 0.6932 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 3s 98ms/step - loss: 0.6620 - acc: 0.0000e+00 - val_loss: 0.8461 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.6564 - acc: 0.0000e+00 - val_loss: 0.6951 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 2s 88ms/step - loss: 0.7070 - acc: 0.0000e+00 - val_loss: 0.7300 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 3s 106ms/step - loss: 0.6713 - acc: 0.0000e+00 - val_loss: 0.7202 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6768 - acc: 0.0000e+00 - val_loss: 0.7062 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 2s 88ms/step - loss: 0.6768 - acc: 0.0000e+00 - val_loss: 0.7346 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 3s 98ms/step - loss: 0.6699 - acc: 0.0000e+00 - val_loss: 0.7065 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 2s 89ms/step - loss: 0.6782 - acc: 0.0000e+00 - val_loss: 0.7129 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6866 - acc: 0.0000e+00 - val_loss: 0.7018 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.7381 - acc: 0.0000e+00 - val_loss: 0.6981 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 3s 92ms/step - loss: 0.7502 - acc: 0.0000e+00 - val_loss: 0.7022 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 2s 89ms/step - loss: 0.6987 - acc: 0.0000e+00 - val_loss: 0.7435 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 3s 92ms/step - loss: 0.7698 - acc: 0.0000e+00 - val_loss: 0.6946 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 3s 106ms/step - loss: 0.7167 - acc: 0.0000e+00 - val_loss: 0.7482 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 3s 91ms/step - loss: 0.7102 - acc: 0.0000e+00 - val_loss: 0.6932 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.7028 - acc: 0.0000e+00 - val_loss: 0.7869 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 3s 104ms/step - loss: 0.6563 - acc: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.6998 - acc: 0.0000e+00 - val_loss: 0.7232 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6805 - acc: 0.0000e+00 - val_loss: 0.7180 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.7590 - acc: 0.0000e+00 - val_loss: 0.7046 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6815 - acc: 0.0000e+00 - val_loss: 0.7989 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 3s 90ms/step - loss: 0.6915 - acc: 0.0000e+00 - val_loss: 0.6932 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 3s 97ms/step - loss: 0.6809 - acc: 0.0000e+00 - val_loss: 0.7466 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6661 - acc: 0.0000e+00 - val_loss: 0.7034 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.6770 - acc: 0.0000e+00 - val_loss: 0.7313 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 3s 96ms/step - loss: 0.6656 - acc: 0.0000e+00 - val_loss: 0.7072 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6789 - acc: 0.0000e+00 - val_loss: 0.7237 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 2s 83ms/step - loss: 0.6652 - acc: 0.0000e+00 - val_loss: 0.7420 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 3s 97ms/step - loss: 0.6646 - acc: 0.0000e+00 - val_loss: 0.7071 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6717 - acc: 0.0000e+00 - val_loss: 0.7039 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 2s 85ms/step - loss: 0.6852 - acc: 0.0000e+00 - val_loss: 0.7114 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 3s 99ms/step - loss: 0.6753 - acc: 0.0000e+00 - val_loss: 0.7524 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 2s 86ms/step - loss: 0.6637 - acc: 0.0000e+00 - val_loss: 0.6973 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 2s 89ms/step - loss: 0.6698 - acc: 0.0000e+00 - val_loss: 0.7245 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 3s 89ms/step - loss: 0.6693 - acc: 0.0000e+00 - val_loss: 0.7104 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6682 - acc: 0.0000e+00 - val_loss: 0.7134 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 2s 89ms/step - loss: 0.6891 - acc: 0.0000e+00 - val_loss: 0.7085 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 3s 100ms/step - loss: 0.6693 - acc: 0.0000e+00 - val_loss: 0.6962 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 3s 92ms/step - loss: 0.6648 - acc: 0.0000e+00 - val_loss: 0.7559 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 2s 87ms/step - loss: 0.6776 - acc: 0.0000e+00 - val_loss: 0.6944 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 3s 92ms/step - loss: 0.6818 - acc: 0.0000e+00 - val_loss: 0.7363 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import Embedding, Conv1D, SpatialDropout1D, GlobalMaxPool1D, MaxPooling1D, Conv2D, MaxPooling2D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import initializers\n",
    "import os\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train, label = read_data()\n",
    "X_train, Y_train, X_val, Y_val = splitData(train, label, 0.3)\n",
    "\n",
    "X_train = X_train[:, :, :, np.newaxis, np.newaxis]\n",
    "X_train = np.transpose(X_train, (0, 3, 2, 1, 4))\n",
    "# print(X_train.shape)\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0], 7500, 4, X_train.shape[-1]))\n",
    "\n",
    "X_val = X_val[:, :, :, np.newaxis, np.newaxis]\n",
    "X_val = np.transpose(X_val, (0, 3, 2, 1, 4))\n",
    "# X_val = np.reshape(X_val, (X_val.shape[0], 7500, 4, X_val.shape[-1]))\n",
    "\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "nb_features = 7500\n",
    "\n",
    "output_dir = 'model_output/conv'\n",
    "batch_size = 5\n",
    "pool_size = (4, 4)\n",
    "# pool_size = 4\n",
    "nb_filter = 32\n",
    "epochs = 100\n",
    "dropout = .25\n",
    "\n",
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
    "bias = bias_initializer = 'zeros'\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=kernel_size, padding='SAME'), input_shape=(1, 4, 7500, 1)))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=kernel_size, padding='SAME')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(4, 4))))\n",
    "model.add(TimeDistributed(Dropout(dropout)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(512)))\n",
    "\n",
    "model.add(TimeDistributed(Dense(35, name='first_dense')))\n",
    "\n",
    "model.add(LSTM(20, return_sequences=True, name=\"lstm_layer\"))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1), name=\"time_distr_dense_one\"))\n",
    "model.add(GlobalAveragePooling1D(name=\"global_avg\"))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir + '/weights.{epoch:02d}.hdf5')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "adam = Adam(lr=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "train_history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                          verbose=1, validation_split=.20, validation_data=(X_val, Y_val),\n",
    "                          callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 7500, 4)\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 611\n",
      "Trainable params: 611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 28 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.4215 - val_loss: 0.2561\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.3913 - val_loss: 0.2298\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 0.3573 - val_loss: 0.2004\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.3190 - val_loss: 0.1675\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.2744 - val_loss: 0.1304\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.2224 - val_loss: 0.0886\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.1647 - val_loss: 0.0453\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0959 - val_loss: 0.0182\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0468 - val_loss: 0.0598\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0604 - val_loss: 0.0631\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0467 - val_loss: 0.0250\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0497 - val_loss: 0.0196\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0494 - val_loss: 0.0224\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0458 - val_loss: 0.0287\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0441 - val_loss: 0.0320\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0371\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0454 - val_loss: 0.0378\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0460 - val_loss: 0.0297\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0449 - val_loss: 0.0302\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0454 - val_loss: 0.0276\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0448 - val_loss: 0.0348\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0451 - val_loss: 0.0316\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0307\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0447 - val_loss: 0.0311\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "train, label = read_data()\n",
    "\n",
    "X_train, Y_train = build_train_data(train, label)\n",
    "\n",
    "# # shuffle the data\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = splitData(train, label, 0.3)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "model = build_LSTM_model(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "train_history = model.fit(X_train, Y_train, epochs=100, batch_size=5, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# One to One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 7500, 4)\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 611\n",
      "Trainable params: 611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 28 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.4192 - val_loss: 0.2523\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.3855 - val_loss: 0.2217\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.3451 - val_loss: 0.1883\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.3027 - val_loss: 0.1513\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.2523 - val_loss: 0.1108\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.1956 - val_loss: 0.0678\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.1271 - val_loss: 0.0288\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0631 - val_loss: 0.0252\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0594 - val_loss: 0.0774\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0540 - val_loss: 0.0413\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0470 - val_loss: 0.0230\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0471 - val_loss: 0.0217\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0468 - val_loss: 0.0232\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0463 - val_loss: 0.0305\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0312\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0447 - val_loss: 0.0336\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0452 - val_loss: 0.0326\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0446 - val_loss: 0.0326\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0447 - val_loss: 0.0290\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0449 - val_loss: 0.0301\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0451 - val_loss: 0.0266\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0447 - val_loss: 0.0276\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0455 - val_loss: 0.0328\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0448 - val_loss: 0.0327\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0442 - val_loss: 0.0283\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0447 - val_loss: 0.0268\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0456 - val_loss: 0.0310\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0290\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0314\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 0.0445 - val_loss: 0.0319\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0444 - val_loss: 0.0325\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0458 - val_loss: 0.0360\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0447 - val_loss: 0.0320\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0275\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 0.0448 - val_loss: 0.0272\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "train, label = read_data()\n",
    "\n",
    "# X_train, Y_train = build_train_data(train, label, 5, 1)\n",
    "\n",
    "# # shuffle the data\n",
    "# X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = splitData(train, label, 0.3)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "model = build_LSTM_model(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "train_history = model.fit(X_train, Y_train, epochs=100, batch_size=5, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VGX2wPHvmZmQAEkgQKih92JoAVGUIqwCdkXF3nFd\nXcva/a1ld9W1rcu6q7jYWBVRBBVUQFHpTYK00DuEQBqQ3vP+/ngnhWQmCTAxTHI+z5MnU+7cee/c\nO+eee+573xFjDEoppWoXR003QCmllO9pcFdKqVpIg7tSStVCGtyVUqoW0uCulFK1kAZ3pZSqhTS4\nq1pDRJwiki4i7app/p1EJL065q2Ur2lwVzXGHYiL/gpFJKvU/RtPdn7GmAJjTLAx5sAptKWLiJS7\n6ENEPhGR593z32OMCa7CvO4SkUUn2walfMlV0w1QdVfpQCki+4C7jDE/epteRFzGmPzfom01qa4s\np6pemrmrM5aIvCAin4vIdBFJA24SkXNEZJWIHBeRwyLypogEuKd3iYgRkQ7u+5+4n58nImkislJE\nOp5Ge07I7kXkThHZ5573HhGZICJnAf8BzncfgSS5p23sbk+i+zVPiYi4n7tLRJa423oUeMG9fD1L\nvVcrEckUkaan2n5Vt2hwV2e6K4FPgUbA50A+8CDQDBgKjAHuqeD1NwDPAE2AA8DffNEoEQkF3gB+\nZ4wJcbdlozFmE3A/sNRdImrmfsnbQAOgE3ABcCdwS6lZngtsBcKBvwAzgJvKLMf3xphkX7Rf1X4a\n3NWZbpkx5htjTKExJssYs8YYs9oYk2+M2QNMAYZX8PqZxphoY0weMA3oV9GbuTPm4j/g2gomN0Af\nEQkyxhw2xmzxMs8A93yeNMakudv9T+DmUpMdMMZMdp83yAL+B9xQlN27p/24orYrVZoGd3WmO1j6\njoj0EJHvROSIiKQCf8Vm8d4cKXU7E6jwhKgxpnHpP2wG7Wm6VOB64D7giIh8KyLdvMy2OeAE9pd6\nbD/QptT9E5bTGLMce5Rynoj0AdoB31XUdqVK0+CuznRle7D8F4gBuhhjQoFnASn3qt+AMWaeMWY0\n0ArY5W4blG9zAlAAtC/1WDvgUOnZeXiLj7ClmZuBGcaYHF+0W9UNGtyVvwkBUoAM9wnHiurt1cZ9\ngvNSEWkA5AIZ2AAOEA9EFJ3odZeEZgIviUiw+6Tuw8AnlbzNx8B4bL39o2pYDFWLaXBX/uYR4FYg\nDZspf15D7XACjwGHgWTsCdH73c8tAHYC8SJSVBb6A3YnsBdYjK2pVxiwjTH7gE1ArjFmhY/br2o5\n0R/rUOrMJSIfAXuMMc/XdFuUf9GLmJQ6Q4lIJ+By4KyabovyP1qWUeoMJCJ/BzYAL53KcApKaVlG\nKaVqIc3clVKqFqq05i4ibbFn9VsChcAUY8y/ykwjwL+AcdgLRW4zxvxa0XybNWtmOnTocIrNVkqp\numnt2rVJxpjwyqarygnVfOARY8yvIhICrBWRBWUutR4LdHX/nQ1Mdv/3qkOHDkRHR1fh7ZVSShUR\nkf2VT1WFsox7zIxf3bfTsIMbtSkz2eXAR8ZaBTQWkVYn2WallFI+clI1d/dQqv2B1WWeasOJY2PE\nUn4HgIhMFJFoEYlOTEw8uZYqpZSqsioHdxEJBmYBD7kHTTrhaQ8vKdcNxxgzxRgTZYyJCg+vtGSk\nlFLqFFXpIib3GBmzgGnGmC89TBILtC11PwKIO9nG5OXlERsbS3Z29sm+VHkRFBREREQEAQEBNd0U\npdRvqCq9ZQR4H9hqjHnDy2RzgPtF5DPsidQUY8zhk21MbGwsISEhdOjQgZJhrNWpMsaQnJxMbGws\nHTue8g8QKaX8UFUy96HYIUc3ich692NPY4csxRjzDjAX2w1yF7Yr5O2n0pjs7GwN7D4kIjRt2hQ9\nv6FU3VNpcDfGLKOS8bKNvcz1Pl80SAO7b+nnqVTd5HdXqGbnFXAkJZu8gsKabopSSp2x/C645+QV\nkJCWTUGh78fEOX78OG+//fZJv27cuHEcP37c5+1RSqlT5XfBHXeZoToGPPMW3AsKCjxMXWLu3Lk0\nbtzY5+1RSqlT5XfjuRdVkKtjLMsnn3yS3bt3069fPwICAggODqZVq1asX7+eLVu2cMUVV3Dw4EGy\ns7N58MEHmThxIlAylEJ6ejpjx47lvPPOY8WKFbRp04bZs2dTv379amitUkp5d8YG9798s5ktcWWv\nlYKCQkN2XgH16zlxnOTJwl6tQ3nu0t5en3/55ZeJiYlh/fr1LFq0iIsvvpiYmJjiboQffPABTZo0\nISsri0GDBnH11VfTtGnTE+axc+dOpk+fzrvvvsu1117LrFmzuOmmm06qnUopdbrO2OB+Jhg8ePAJ\n/cPffPNNvvrqKwAOHjzIzp07ywX3jh070q9fPwAGDhzIvn37frP2KqVUkTM2uHvLsNOz89mTlE6n\nZsEEB1Vv8xs2bFh8e9GiRfz444+sXLmSBg0aMGLECI9X0gYGBhbfdjqdZGVlVWsblVLKE787oVpU\niTHVUHUPCQkhLS3N43MpKSmEhYXRoEEDtm3bxqpVq3z+/kop5StnbOZeE5o2bcrQoUPp06cP9evX\np0WLFsXPjRkzhnfeeYfIyEi6d+/OkCFDarClSilVsRr7DdWoqChT9sc6tm7dSs+ePSt8XWZuPrsS\n0unQtCGh9XUwrKqoyueqlPIPIrLWGBNV2XT+V5Zx/9ef9VZKKe/8L7gXF901vCullDd+F9yLaGhX\nSinv/C64a1lGKaUq53/BXasySilVKb8L7pq7K6VU5fwuuJ9JmXtwcDAAcXFxjB8/3uM0I0aMoGyX\nz7ImTZpEZmZm8X0dQlgpdbr8L7i7/58Bsb1Y69atmTlz5im/vmxw1yGElVKny++CO9WYuT/xxBMn\njOf+/PPP85e//IVRo0YxYMAAzjrrLGbPnl3udfv27aNPnz4AZGVlMWHCBCIjI7nuuutOGFvm3nvv\nJSoqit69e/Pcc88BdjCyuLg4Ro4cyciRIwE7hHBSUhIAb7zxBn369KFPnz5MmjSp+P169uzJ3Xff\nTe/evbnwwgt1DBul1AnO3OEH5j0JRzaVe9iJoVNOAfVcDnCe5L6p5Vkw9mWvT0+YMIGHHnqIP/zh\nDwDMmDGD+fPn8/DDDxMaGkpSUhJDhgzhsssu8/rbpJMnT6ZBgwZs3LiRjRs3MmDAgOLnXnzxRZo0\naUJBQQGjRo1i48aNPPDAA7zxxhssXLiQZs2anTCvtWvX8uGHH7J69WqMMZx99tkMHz6csLAwHVpY\nKVUh/8vcq1H//v1JSEggLi6ODRs2EBYWRqtWrXj66aeJjIxk9OjRHDp0iPj4eK/zWLJkSXGQjYyM\nJDIysvi5GTNmMGDAAPr378/mzZvZsmVLhe1ZtmwZV155JQ0bNiQ4OJirrrqKpUuXAjq0sFKqYmdu\n5u4twzaGPYdSaBEaRIvQIJ+/7fjx45k5cyZHjhxhwoQJTJs2jcTERNauXUtAQAAdOnTwONRvaZ6y\n+r179/L666+zZs0awsLCuO222yqdT0Xj/ujQwkqpilSauYvIByKSICIxXp5vJCLfiMgGEdksIrf7\nvpm/nQkTJvDZZ58xc+ZMxo8fT0pKCs2bNycgIICFCxeyf//+Cl8/bNgwpk2bBkBMTAwbN24EIDU1\nlYYNG9KoUSPi4+OZN29e8Wu8DTU8bNgwvv76azIzM8nIyOCrr77i/PPP9+HSKqVqq6pk7lOB/wAf\neXn+PmCLMeZSEQkHtovINGNMro/aeAIRQZBq6wrZu3dv0tLSaNOmDa1ateLGG2/k0ksvJSoqin79\n+tGjR48KX3/vvfdy++23ExkZSb9+/Rg8eDAAffv2pX///vTu3ZtOnToxdOjQ4tdMnDiRsWPH0qpV\nKxYuXFj8+IABA7jtttuK53HXXXfRv39/LcEopSpVpSF/RaQD8K0xpo+H554C2mKDfAdgAdDNGFNY\n0TxPdchfgE2HUmgWXI9WjfSHp6tCh/xVqvb4LYf8/Q/QE4gDNgEPegvsIjJRRKJFJDoxMfGU31A4\nMy5iUkqpM5UvgvtFwHqgNdAP+I+IhHqa0BgzxRgTZYyJCg8PP+U39NILUSmllJsvgvvtwJfG2gXs\nBSouTFegSmWiaqy51zY19UtbSqma5YvgfgAYBSAiLYDuwJ5TmVFQUBDJycmVBySpnh/Irm2MMSQn\nJxMU5Psuo0qpM1ulvWVEZDowAmgmIrHAc0AAgDHmHeBvwFQR2YQthz9hjEk6lcZEREQQGxtLZfX4\n+JRsjgc4SGtQ71Tepk4JCgoiIiKippuhlPqNVRrcjTHXV/J8HHChLxoTEBBAx44dK53u7ld+ZnDH\nJrxxrfYAUUopT/xy+AGXQygo1LKMUkp545fB3ekQ8jW4K6WUV34Z3F0OBwUFGtyVUsobvwzumrkr\npVTF/DK4u5xCQWGFoxsopVSd5pfBXTN3pZSqmF8Gd+0to5RSFfPL4K6Zu1JKVcwvg7vL4dDMXSml\nKuCXwV0zd6WUqphfBndbc9feMkop5Y1fBnenQ8jXi5iUUsorvwzutp+7BnellPLGL4O7U0+oKqVU\nhfwyuLv0hKpSSlXIL4O7Uy9iUkqpCvllcLeZu/aWUUopb/wyuGvmrpRSFfPL4K41d6WUqphfBnen\n/liHUkpVyC+Du8upmbtSSlWk0uAuIh+ISIKIxFQwzQgRWS8im0VksW+bWJ6eUFVKqYpVJXOfCozx\n9qSINAbeBi4zxvQGrvFN07zTmrtSSlWs0uBujFkCHK1gkhuAL40xB9zTJ/iobV45HQ6MgUIN8Eop\n5ZEvau7dgDARWSQia0XkFm8TishEEYkWkejExMRTfkOXUwA0e1dKKS98EdxdwEDgYuAi4BkR6eZp\nQmPMFGNMlDEmKjw8/JTf0OmwwV37uiullGcuH8wjFkgyxmQAGSKyBOgL7PDBvD1yOYoy90LAWV1v\no5RSfssXmfts4HwRcYlIA+BsYKsP5uuVZu5KKVWxSjN3EZkOjACaiUgs8BwQAGCMeccYs1VE5gMb\ngULgPWOM126TvlCSuWtwV0opTyoN7saY66swzWvAaz5pURU4HfaAQzN3pZTyzD+vUNXMXSmlKuSX\nwb245q7jyyillEd+GdxL+rnrEARKKeWJXwZ37S2jlFIV88vgrjV3pZSqmF8Gd+0to5RSFfPL4K6Z\nu1JKVcwvg3tJzV1PqCqllCd+GdyLM3ftCqmUUh75ZXDX3jJKKVUxvwzuOp67UkpVzC+Du/aWUUqp\nivllcNfeMkopVTG/DO7aW0YppSrml8FdM3ellKqYXwZ37S2jlFIV88vg7nKfUNV+7kop5Zl/Bncd\n8lcppSrkn8Fda+5KKVUhvwzuWnNXSqmK+WVw15q7UkpVzC+Du9OpmbtSSlWk0uAuIh+ISIKIxFQy\n3SARKRCR8b5rnmdac1dKqYpVJXOfCoypaAIRcQKvAN/7oE2V0itUlVKqYpUGd2PMEuBoJZP9EZgF\nJPiiUZVximbuSilVkdOuuYtIG+BK4J0qTDtRRKJFJDoxMfGU39PhEByiNXellPLGFydUJwFPGGMK\nKpvQGDPFGBNljIkKDw8/rTd1ORyauSullBcuH8wjCvhMbKmkGTBORPKNMV/7YN5eOR2imbtSSnlx\n2sHdGNOx6LaITAW+re7ADrbHjPZzV0opzyoN7iIyHRgBNBORWOA5IADAGFNpnb26OJ2ivWWUUsqL\nSoO7Meb6qs7MGHPbabXmJLgcojV3pZTywi+vUAWtuSulVEX8NrhrbxmllPLOb4O7Zu5KKeWd3wZ3\nrbkrpZR3fhvcbeauvWWUUsoTvw7u2s9dKaU889vg7nJqzV0ppbzx2+Du1N4ySinlld8Gd5f2llFK\nKa/8Nrg7HUK+nlBVSimP/Da4a+aulFLe+W9wd2rNXSmlvPHf4K6Zu1JKeeW3wd3pEPK0n7tSSnnk\nt8HdpVeoKqWUV34b3J06toxSSnnlt8Fda+5KKeWd3wZ3p8OhY8sopZQXfhvcNXNXSinv/Da4O51a\nc1dKKW/8NrhrbxmllPKu0uAuIh+ISIKIxHh5/kYR2ej+WyEifX3fzPK0t4xSSnlXlcx9KjCmguf3\nAsONMZHA34ApPmhXpbTmrpRS3rkqm8AYs0REOlTw/IpSd1cBEaffrMrpeO5KKeWdr2vudwLzvD0p\nIhNFJFpEohMTE0/rjTRzV0op73wW3EVkJDa4P+FtGmPMFGNMlDEmKjw8/LTez+kO7sZogFdKqbJ8\nEtxFJBJ4D7jcGJPsi3lWxuUQAM3elVLKg9MO7iLSDvgSuNkYs+P0m1Q1TqcN7lp3V0qp8io9oSoi\n04ERQDMRiQWeAwIAjDHvAM8CTYG3RQQg3xgTVV0NLqKZu1JKeVeV3jLXV/L8XcBdPmtRFTkd9qBD\nM3ellCrPr69QBc3clVLKE78N7k5HUc1dhyBQSqmy/Da4a+aulFLe+W1wL87cdUx3pZQqx2+Du8up\nmbtSSnnjt8Fde8sopZR3fhvcteaulFLe+X1w194ySilVnv8Gd625K6WUV34b3LXmrpRS3vltcHdp\nV0illPLKb4O7XqGqlFLe+W1w194ySinlnd8G95LMXYO7UkqV5bfB3eU+oVqgNXellCrHb4O7Zu5K\nKeWd3wZ37eeulFLe+W1w194ySinlnd8Gd+0to5RS3vltcNeau1JKeee3wb24t4wGd6WUKqfS4C4i\nH4hIgojEeHleRORNEdklIhtFZIDvm1meZu5KKeVdVTL3qcCYCp4fC3R1/00EJp9+sypXXHMv0BOq\nSilVVqXB3RizBDhawSSXAx8ZaxXQWERa+aqB3jidmrkrpZQ3vqi5twEOlrof636sWmlvGaWU8s4X\nwV08POYx4orIRBGJFpHoxMTE03pTrbkrpZR3vgjusUDbUvcjgDhPExpjphhjoowxUeHh4af1ptpb\nRimlvPNFcJ8D3OLuNTMESDHGHPbBfCvkTtw1c1dKKQ9clU0gItOBEUAzEYkFngMCAIwx7wBzgXHA\nLiATuL26GlumXbgcQoEOP6CUUuVUGtyNMddX8rwB7vNZi06C0yGauSullAd+e4Uq2B4zOp67UkqV\n59fBXTN3pZTyzK+De4DTob1llFLKA78O7pq5K6WUZ34d3LW3jFJKeebXwd3p1MxdKaU88evg7nJo\nzV0ppTzx6+DudAj52hVSKaXK8evg7nKI/kC2Ukp54NfB3ekQLcsopZQHfh3cXdoVUimlPPLr4K6Z\nu1JKeebXwd3lcOgJVaWU8sCvg7tm7kop5ZlfB3eXU3vL1BkZyfD1HyAnraZbopRf8Ovgrpl7HbJn\nIayfBgdX13RLlPILfh3ctbdMHZJy0P0/tmbboZSf8Ovgrpl7HVIU1GtrcF/0MuxbVtOtULWIXwd3\nl8OhmXtdcbwWZ+75OTa4r/ukpluiahG/Du6audchtTlzP34QMHBsX023RNUifh3cdWyZOqQ2B/ei\noK7BXfmQXwd3p/5Adt2QnQI5KeAKgtRDUNt26Mf22v9phyEvq2bbomqNKgV3ERkjIttFZJeIPOnh\n+XYislBE1onIRhEZ5/umlufSH+uoG4qy9TYDoSAXMhJrtj2+dnx/qdsHaq4dqlapNLiLiBN4CxgL\n9AKuF5FeZSb7MzDDGNMfmAC87euGeqI19zqiKLi3O+fE+7XFsX2A2NtH99ZkS1QtUpXMfTCwyxiz\nxxiTC3wGXF5mGgOEum83AuJ810TvtLdMHVHUx704uB+subZUh2P7oHW/ktt1RephWDetpltRa1Ul\nuLcBSn+bYt2PlfY8cJOIxAJzgT96mpGITBSRaBGJTkw8/UNrzdzriJRYcARAmwEl92sLY+DYfmgT\nBfWC61ZwXz0ZZv+hdq3PM0hVgrt4eKxsRL0emGqMiQDGAR+LSLl5G2OmGGOijDFR4eHhJ9/aMrS3\nTB2REguN2kD9MKgXUruCQdYxyEmFJh0hrIP/Bfdf3oWt357aa2PX2v+HN/quPapYVYJ7LNC21P0I\nypdd7gRmABhjVgJBQDNfNLAiLqdm7rXBL3uP8tzsGIzxsi6PH4RGbUEEGkXUrrJMUTBv3N7/gnth\nIfz0V1j6j1N4bQHErbO3j2hwrw5VCe5rgK4i0lFE6mFPmM4pM80BYBSAiPTEBvdq79Lg1Jp7rfDR\nyn38b+V+th3xMuJjSqwN6mAz+NqUuRcF87AOJcHd207uN5adV+B9hwuQtMMedRzZBHnZJzfzhK2Q\nl2Fva+ZeLSoN7saYfOB+4HtgK7ZXzGYR+auIXOae7BHgbhHZAEwHbjMVbhW+4XIIxkBhXQ7w9gOo\n6VacMmMMq/YcBeCnrfHlJyjIh7S4UsE9opYGd3fmnp8F6R4+h99YSlYeg1/8kWmrK+iaeSja/i/M\nO/nsu+i1rfvbnYPyuSr1czfGzDXGdDPGdDbGvOh+7FljzBz37S3GmKHGmL7GmH7GmB+qs9FFnA57\nOsBj9p643QaG2m72ffDxFRVPs2cRfH4zFOT9Jk06GXuSMkhKzwHgx60JkLAN5j9lD9vBXthjCm1Z\nBmxwz0zy3cU+qXGQdsQ38zoVx/dDg2YQGGKDO5wRpZnlu5JIzc7no5X7vGfvsdH2wrKi21VwJCWb\nRdsT7PT1w6DX5ZByADKP+qbhqphfX6Hqcgf3cnX3PYvhrcGw9sMaaJUXidvhta6wb3nF0yXttANJ\nVUVuJsR8CXsXlwys5cni12DrHNj8ddXb+xtZ7c7ar+zfhg2xx8la8iasehsOrLITFNXXizN3d5BP\nOXT6b24MfHK1/aupUsixfSVBPaxjyWO/oey8gnKPLdqeAMCO+HQ2xqZ4fmFsNLQbAqERELumSu/1\nxoLt3DF1DQWxa+1Faa362idqMntP3A57l/p2nhlJ8K++sHuhb+d7Evw6uJdk7qXKEvk58N2f7O0t\ns2ugVV5EfwgZCTD/Se9llMMb7E7pu0eqNs/dP9vDeIBtXnosJO+G/e6hZFe9dcbUc4us2pNMeEgg\nd53fETGFyI559ontc+3/ohJM6cwdfHNSNW4dJGyB+BjYv+L05hUbDVNG2s/bm7QjMCkSts8reezY\nPluSAWjcFpDfLLgnpefw6Bcb6PXsfBbvcJ8iy8vGGMPiHYmc37UZQQEOvlizH2bdBdvnl7w4NwMS\nNtsunBFRJWWWSqzYnUyQycaRuM2+tmVRcC9V1sk6bk/Snmwd/1QUFsIXt8H0CXaZfGXzV3Y9/vo/\n383zJPlfcN+7FN4bDdkpnjP35f+C5F3QfijsX25/nq2m5efAxs8gpLXdiDd9UX6awgL49mFbgtj4\neflSQUF++cC87VsIagzhPWBL2XPcbus+BnHAsMdsMCvKiCtzMjuBwgL45kH49aOqv6YgD5N1jNV7\nkzm7YxN6tQpldMgBgnKP2v7e276zbSjO3N2XVhQHd3fQLyyETTNPrbSyYTo4A+1n+MuUqr3mwCr4\n5qETj66Mge+fhrhf4YdnvL926T9sGeaXd+39gnx7xFWUubsCIbRNtV+laozho5X7GPn6ImavP0T9\nACcfrdgHSbvg1Y4cWjaN+NQcLu3bmrF9WnFo4892m13yWslM4tbbbTVikA3uxw9AekKF73vwaCax\nx7KIdOxBKLSZe8OmdplLn1T95V3bC2fdxye/cGnx9kisqtvitm/sDj433W5zvlJ0lLzje3uEXVr6\nbzN8hv8F98Bgewi4fjpOp21+XtHgYcm7Ycnr0PsquOhFu/HtKJUl5efaHcNHl8PWb367mvz2ubY/\n82X/hlb97IZbtma8diocWgsjnra18dX/LXkuNwP+O8z+hmiRgjybAXYfC72vhAMr7YZdWkE+rJ8O\nXS+E8x62QWzVW5W3N34LvN4Vfq3il2v5JNv+7x6tOHMtbfZ9FPx7MKmpKQzp1BQR4eawGHKNk7yh\nj9jBtBK32SDeoCnUa2hfF9IaEDuAGMCO+TDrTph87okZcWXyc+1Oocc4GHCz3R5SK7mwOicdZt5p\ny32lu//t+tH+/F/r/rD9O3uOo6zjB+zRW2Co/cnAtCOQGgumoCS4Q/nukIWFle9ot34L858uOU9R\nie82HebZ2ZuJjGjEvAeHceu5HVi4PYGsn16BvEwCfpkMwPBu4VwzMIKLC362LzwUbcuGRbfBBvaI\nQfZ2JXX3FbuT7HwbusfSaTPQ/m95Vknmboz9OUWA5W+WP090dK/3I9/k3fDBhXZ9zHvSXgFbkcJC\nWPwqNO0CjdvZnb0vpB62iWX78yAv07anSF4WvDMUFjzrm/eqgP8F99b97ca05l1cYjf6gkJjN4q5\nj4KzHlz0kg2ijdqeuDfe+LndMRyJgc9vgklneb4Ao7DQ7l1T4+zVg54OD6M/gDd6wdvnwP8us4Et\n67jnNv/6sa1Ldh4JF75gv9Sr3yl5Pj0BfvwLdBwGwx+HnpdC9PslPwa94Dl7CLzh05Iv0P7lkH0c\nelwCPS8DTPnSzK4FkH4E+t9sg2PU7fbzqOywf8GzdnCubx60pZ+KxEbDzy/aHYgryH30UUkw2r8S\nNn6OKzOBG5w/M6RTEwAGZq1kVWEvfgkdbafbPtfdxz2i5LWuehDSsiSjXz3ZBvzQ1vbQ+rtHq3Zy\nbucPkHUU+t4AUXfaRGDt1Ipf8/MLdqfSdogN7vGb7bL+/DfbT/3Wb2yQ8BRoF79ij6Cu/ci+V8ws\nu22B9+BeWAhTx8Gb/WHt/+wOqaykXfDl3XanveLNypcbmP7LAdo0rs/Hd5xNl+bBXDeoLRHEE7h1\nFjRqR4u0GC4Lj6dFaBBDIgIZ5/yFX4POtu3f8JmdSWy0XeaGzWzd3OGqtDSzcncyzYIDuajRIfYV\ntiChwL3Dbhlpu1XmZtry2LG9NmFJOWDLG0ViZsGb/WDe4+VnHrcePrgIslPhqvdsD56f/lrxB7Fj\nni3JDXsMIq+zO+XSO4SC/FPrmbVlNmBg3Gs2MSldHl73ie0N1eV3Jz/fk+R/wR1g0N2QvIvWR+2P\nJecXur8su3+GC/4Moa3sBS89LraP5WbYL9uyf9oN6ZHtMGG6PST86p4TD4PzsuF/l8DrXeCNnvCv\nSPjPoBM3/YWlAAAgAElEQVRP4MVGw9zHILg5NOlk98ZrP4R3L7AnZ0pLibVt6HcDOJzQ8XzoNhaW\nvgGrp9jAP+ePtnZ+8Ru23UMftMPcrvvEnpBZ8y4MvM32qvjxeRtQtn4LrvrQ+QJo3hOadLYnTUv7\n9SNo2By6XWTvD55ov6CljwrK2rPY7hSGPW7LPTNutX2SPclOtVlzaGu46l0Y/aw9ueup7FSksMB+\nOUPbsKf+WdwTMJfOYQGQuIMGaXtZLIOYv99hd+Lb5rr7uLc9cR5F3SGPxMDeJXD2RLjrJzjnfvtZ\nvdYZ3vudLSOUPZopsmG6/Ww6X2CvDu16oc2sPQVQsFdTrn4HBt0J10+3R0Gz74ctX9tzJSOesj1e\nfvc3uyMuXRZI2mWPoAbdaXfwrfrZRKN0H/ciTTrYHXJupt2ZH1hpH//mAfj3ADsWS9HOMz/Xfv6u\nQBssfn4BDv3q/bMH9idnsHxXMtcNaovDXdZs37Qhfwn7njwcpF3zOZkmkLuC7E7dsXUODcjmpdQx\nZLcbbttdWGi/AxFRdqYB9aFFn5KTqsbArLvh1U42ifrlXcyxfazYncy5nZvSNnML601nVux2l0xb\nRdodXsJWm7XXC4HL/gPhPWHZpJIhGr55CAIb2XW85r2Shdr2HUy92CYXd3wPkdfAkD+4k6G1nj8I\nY+wON6wj9BkPkRNsG4q2XWNsbJh0lt1JeNsuPNn8JTTvDS162eRrx3wbV/Jzbdm47RDocF7V53eK\n/DO4974CGjSj6357GGUyj9oTla0HwOC7S6brcTHkZ9vDoq1z4OhuOP8RcLrs4fiE6TbYffV7G3SM\ngbmP2Kx4+BNwySQY97otqXxytf2fnQIz77DZ4s1fwYRpcNcCm7XlpMK7o2xQKrL+U8BA/xtLHvvd\nX22gn/cYzLnfrvxhj0Ozrvb5iCg7SNbKt2wAadoVxrxs27RvqV2ebd9Cl1FQr4HdIfS6zJ6PKMpa\n047Yel+/68EZYB8LbW0zougP7Zdv5dv2S1oULAoLbdbeqK39nG743H5xp11rg9WO723w2PUjrHkf\nZtxiyw1Xvwf1G8PA2+2h9vdPe8+e10+DIxsxo//CpPyraM5RZMOntpwBpHf4HT9sOcKepsNtJnh0\nt/fgvnqy3cENuNUGuItehHuW2kysMM8Gu8nnlC/XZB61yxJ5rd0WwG43GQm2zrtnsf1slv7DLmta\nvD2KCWkJo56FBk1g7Cu2xv7V76FZNzsvsF372p1r33vN+/Z3UX/+q23fee4T/ZHXweEN5G+bh3G4\nbM25SFGPmfjN9miu7dnwx1/hxlkQ3MKOxTL9enu0t+jvcHi9Lfdd/S4Et7TBPifdrssDq2DjjBNK\ngDOiD+IQuCaq1NHQ8QMMz/qR6fkjeTW6kK8LhtLn6A92e18/jbzGnVhruvHqkf6QcpCDSz+y1x4U\nlWOKttlD6+z3aOMM2DQDmveyGfXcR+HfUVyT+TmjW+cQkHmE7c7uxWUaWkba/wdW2Fp17yts+fW8\nh+yOcvs8e0IX4J5Fdkc893Gb+Cx5DT67wX537vwBwrvZ6YY9aj+veY97LuPs+N7ulIc9areBZl3s\n8myYbr8Pv34EMTNt25b+A94fbb8re5fa78/i12DnAhsPSkuJtSW6PleWbA+56TbB2/i5PeIc9qj9\nzlYzV7W/Q3VwBcLA22i19B9EyBU0WvaC/cLe9KUNmkXanQv1m9gsN3GrDZI9Ly15vnFbe+j01T32\nkLZesM2Whz0OI58uma5ZN5g23n6pglvYFXjHfNtPt0j7c2HiIvjsRvjsertR9LvRzq/jsBOzs/Bu\n8KdttuySn2U3pqIeE0XOfcDORxxw5wIbZAfeZg+/v77Xlk1KL0vPy+yRyfa50KQzZsmriCmwJZnS\nRj1rv+z7ltkvIEDnUTD2VRsoDq+HK/8LAUH287nhc3uOYo6HseCc9Wym2m6Ive9w2h3ilBHw4Ti7\n82k3hPzw3mxJa8Cve+O5buVzuNoM5nDrccxJW8gzLfoQvmySPXxt1Y+xQ6OY9b9o7o1uyfeBQEEu\nmfVb0aD0+4a2sdna8YN2p9mgSclzrSLt38inbZ/5L++y5ZpBd8O590NQIxt8CvOg7/Ulr+s8ygbW\nop5Wnlw3zb4eoM/Vtma/Y559r6LtTgTGvWo/s1Lz2t19Ip2D3eMpnTUe88Ofce2cR0pQBI1Kb7NF\n28l3D0NGAtHnTiZv7zHO6TraHmWsfscevb012JYBB9xSsh1c/a7NYKeOs+WFDPcJzkZtYdSz5Pe6\nii+iYxnRvTmtGtUvec9lkxCHg08DrmLHqv1EBY7hhoKfbblt/3ICLniG/zQeyIwVwaQdnkzAz8+D\nwIzDLTj3WCYRYQ3cpdL3bMCd+5jNTm+Zbbffo3vYP+NJHoufQcFq2zWwsM1Alu9KxhiDNG5nj4SW\nTbJXrfa/qeQz/vkFu8PKy4Sr37dHyle/D+9f6O7CWmB3lpf+y35HigSGwKjn7M5w3uP2O9isGyRt\ntwnXzgW2rBR5Xclr+k6wPdU2zrCv6TTSxpTt38GcB+C9UeW3CXHYstTwJ6H7mJITqb2vsv87DrPL\ntvlLe06tVV/oMtr7NuZD8htcSOpRVFSUiY6uWvcpj1JiMf88i+UFPTnPudmWMn7nocb29R9sndAU\nwOVvn5hBgw2sM25xZ3fGfvATpoOjzEFNzJc2Y8fYAHm+l+6KeVm21LJ+mg2UYGuAkdec3PIVFsLn\nN9peP+feX/L4xi9swHK44LFdJTsYY2w3u/R4KMgh2xnCh3I5tz3+JvXrOT2/R9oRW85a9LI9wqkX\nbAPnPUtOXP78HDvftHgbMOqH2S9GSKvynxPYEsTaqZi4X5GCksPZTBNIELlclf8CeS36sjkulWVX\n5BAx/3Y7wcj/g+GPk5GTz6bY4/T8YhiNsg/xfa9Xuejae0rmv+odmP8EAD9e8A0v/VLIpX1bc9/I\nLtRzlWlPfo49rF75nxMfb3EW3LvsxMdio23W1bwXtOhtk4jDG2wvI2cgDPn9idNnHrUnR3tdWf5z\nKCyE1EP8a8ZcYvfv4kfHeUy/byQ9WoaSmp3H1tcu5OyCX1klfRn450UEuDsHkJFky0pAfuT19N9w\nJU6nsPjRkTRq4D4CS9hmE5L8HLjrR5vlFln8mj307zLKBv36je3yH95AWqPuzEzuwKAhw+jT6yxb\n545bZ0sR/W7kr0zkg+V7GdO7Je/kPmU/CwQejik+75Ez614CN31KPi56Z79HDvV4eHQ3Huwn8J+B\ntmxSmMf+a38gMaANUR3sjvfeT9bSdP9c/hbwIZKXzSfDF/Lnb3ex+LERtG/aEKZeYo9Km3SGP64t\nyWxX/9cG2r43wJWTS5bz2D77ve0zHs79o+dMuLDQJkg75p/4eEgrG9QH3eXuflpqfb7eze74GzaH\ne5fb0ivYbX/XAvv9aNrFfgcOrbXnCGJmQfJO6H6xbZfDCb8v1W/+6/tgvfvHz6/92B5lnwYRWWuM\niap0Or8N7sCRd6+h5aEfyA1pR70/rrYlirK2fWcP2xq1hQfWlZQoSstItofvgSFw988l2VlZ66fb\nQ/Exr3gOauUaGGNLC/1uKjn8P12FhfYQMbiFrf2WtmoybPqCo92vY+T3zUkpqMezl/TijvM6Fk9i\njCHmUCrrY48TE5tCQlo2r41tSbOVL9kv+U2zoNOI026mMYa/fLWOjWsWc3X7LAY2yaJdQCrpjbry\nTsYFzPo1luBAF0sfG4Fjyvn2xNbvl0PLPiUzmfckrJ7MXfVe4b9P3lN8XQNbv4XPb2RTUBSXHv8T\nbRrX59DxLLq1CObV8X3p17Zx+QYdWmtrutmp9oip6+iS3hrVZO3+Y1w9eQW3nNOeeTFHaFDPyZz7\nzuOprzYSuHUW/3S9xbT8UTS+9i0ujmxV9MHB320g/WbYN/zxW3uC767zOvLnS0r9Ro4xtgRSle2q\n0NaSd8/9J61z9lCfUt04G4bb0s/Fb7AzswFj/rWU18ZHcpVzOXw10R4t3FzqpOa+ZfbooPUADo7/\njufmbGbl7mRWPjmSxv/uBtnHKRz3D0Yv6cLe5AxeuSqS8QMjGPjCAkb1bMHrF7eFjCR2mdaMfmMx\nL115Fjec3Y7C+U/jWPUWXPCMLVsUyc+1J1V7XlLSY+pk5aTbHVnSDru8nUaceIRf2mc32phxy9dV\n/x7k59oL7xbb3kaMft72Tiuy4wf49Bp7DuvelVWLHRWoE8F9zZK5RP50C3FjP6TjkEvJzS8sn7nl\nZdluhOf9ydafvclIsplaYMhptem3YPKyEHHaniMe/HH6OhZsOULn8GCS0nNY/NhIggLsxvzmTzt5\nY8EOAMIaBJCRU8DIHuG8c9NAm2W7Ak+/fcbw4ndbeW/ZXu4Z1oknx/ZAymRW2XkF5BUUEhIUYLOf\nLXNgzN9PzMCSdnL4qz8zYvcE3rrlXEb3agHAzm0xtJl+Ab8vfILhF13Fbed2YPGOBP7vqxjiU7N5\n+apIrh1Upk7/GzPGcM07K9l/NJPFj41gS1wqE6asomWjIGKPZfHMhe25Y/11vJh1FZvDL2b6xCEl\nL170MjTpxFXL2pCSlcfA9mF8te4QPzw8nI7NTi3A7YhPY8ykJdwzrCNPDA60/e2bdbOZaKnP/EhK\nNi1CA5H8HPjiVjjnPltaKFJYCFOG2zLg8MfYejiVsf9aypNje/D71H9D1lHm93yZ309bR4emDdiX\nnMnNQ9rz8ar9vHFtX64aEFH8+Qz5+08MaBfGRb1bsuqHz3ky83VeaPc+d44bSs9WoWUXwafSsvPI\nKzA0aVjmO5SRZLt7tj+nSvNZf/A4r3+/nSfG9OCs4FRbsx880R4xFcnPhU+vtZ9l19PvJVMngvvC\n7QlM/HAlz1zel9V7jzJv02GaNKxHz1ah9GwVSvOQQEKCXDSqH0Dn8GA6hQcXZ3/HMnLZHJdKt5bB\nNA8J8jh/YwxzNsRxPDOPm4e0L+5dUBX5BYXM2RDHj1vjefTC7nQKD678RWXee/b6OHYlpFNgDIWF\nhsMp2exOTGdPYgZ92zbivzdFlRyqu204eJzL31rO/SO7MKRTU256fzUvXNGHm4a0Z92BY4x/ZyVj\n+rTkqbE9aNO4Pv9dsoeX523j39f359K+rU+qjdl5BdRzOk74XHLzC3ll/jbeX7aX287twHOX9ioX\n2E9GXkEh573yM91ahPDxnWeTnVfA5f9ZzrGMbGbeex7tmpYcraVl5/GHab+yes9RZvz+HM8ZfAUO\nHs0kPSe/yoHFGEN6Tr7dQZXx/eYj3PPx2uLMFODjVft55usYRvdszpSbo3AIvL14N6/O386PfxpG\nl+YlicW2I6mMmbSUP1/ck8v6tmbE64s4v2sz/ntzpd/pYgWFhsU7Evho5X4W70ikfoCT+Q8OO+Ez\n84Xrp6xif3IGSx4fidMhXPH2Co5l5PL9Q8N44LN1LNhieyytfOqCE2r9f/p8PV+us73QurcIYXi3\nZkxfc5D0nHwu69uaZy7pRbPgqiUbuxLSaN24Pg3qVXwkU/S9em7OZjJz8xk/MIJ7h3c5pc8k5lAK\nN7y7itTsfEKCXHx859let7mMnHw2xqawIfY4Gw4eZ1TPFowfGOFx2spUNbj75wlVN5dDyMPFs7M3\nExLk4uYh7cnILWDr4VSmLt9HbsGJZ8kb1HPSo2UIyRm57E+2V401bhDAmxP6M6zbiT8ekpCazdNf\nbbKDWQHLdiXxxrV9y32RCwoNe5MyOHA0A6fDQYBDOHA0k8mLd7M/OROHwKo9R5l6+yAiI0pW/OGU\nLBLTckjNyicjN5+B7cOKN+TM3HyemLWJbzbE4RA7zIJDhGbBgXRuHkxkRGNmrY1lwrur+PjOwcWv\nM8bw0tytNG1Yj3uGdyI40EX/do2ZvGg3l/ZtzZ9mbKBlaBB/v+osQt3Lcdd5HZkXc4RnZ8dwTuem\nlX6ZDqdk8eOWeOZvPsKqPUdp36QBfxjZhcv7tWbr4VQe+2Ij2+PTuPWc9qcd2AECnA5uPLs9byzY\nwZ7EdD5dfYDt8Wl8ePugcl/IkKAA/n19fy759zLu/WQt3/zxvOLlySsoJHrfMX7eFs/SnUl0Cm/I\nM5f0Kg42326M4/GZG8nMLeDqARE8MaY7zUM97/QBktNzeGLWJn7eFs/EYZ15aHTX4qOjtOw8Xpm/\njc7hDbm2VK+Um85uR5fwYPq2bVS8Q7w2qi3/XLCDT1Yd4PnLehdP+9kvB6nndHD1gAjCGtbjDyM6\n8/oPO5i6fC+HU7JZujOJA0dLrnxsGlyPMb1bcnFkK1o1qs+M6INM/+UAsceyaB4SyAMXdOWGs9vR\nooJlOlW3D+3AxI/X8sOWeJo0rMeGg8f52xV9qF/Pyds3DuCZr2NISMs58SQuMGFwOxLTc7jx7HZc\n2KslDodw38iu/HfJbt5btpeVu5OZdF0/zu3SjIycfN5ftpcv1h5kZPfmPDS6G00a1iM1O4+XvtvK\nZ2sO0q5JA14bH8nZnZp6bGd8ajb/91UMP26NZ0C7xvRsFcoX0bHMiI7l6gFtePTCitd5aTvi07j5\n/dWEBAXw3q2DePSLDdz83mqm3jGYge1LOlrkFxQydcU+/vHDDrLcY/i0b9qAIV7a6Et+nbknp+fw\n8IwNDO8WznWD2hIcWLKvKiw0pOfmk5qVx/HMPLYfSWPToRS2xKUS1jCAfm3D6BzekH/8sIMdCWk8\n8rtu3DykA5sPp7D+4HGmLNlDVm4Bj13UHYcIL87dSoemDfjzxb2IS8li+5E0th5OZXNcKpm55a8M\n7NMmlAcu6EqX5sHc8sEvHMvIZdKE/hzLzOWzXw7w64ETL3iq53Qw7qyWXBzZmn/8sJ3t8Wk8flEP\nfj+8k8cAuWRHIhM/jqZ1o/q8fHUkadl5bIhN4c2fdvK3y3tz8zkdAPh5Wzx3TI2mS/NgdiemM/3u\nIeU2rJ3xaVz85jJGdA/nnuGdSMnKIzUrn5z8AnILDFm5+cQcSmXt/mMcOm671XUKb8gF3ZuzYncy\nWw6n0jI0iIS0bMJDAnnpyrMY1bPFaa3b0hLSshn68s/0bxfGL3uPcvOQ9vztij5ep485lMLVk1fQ\nv11j7hjakfkxR/hxazyp2fnUczoY0L4x6w8ex+Vw8PiY7uxNyuDD5fsY2D6MqPZhfLh8HwFO4aoB\nETSo58ThEIIDXXRvEULP1qHsOJLGYzM3kpqdx9DOTVm4PZFO4Q15aHQ3VuxKYs6GODJzC3j/1qgq\nfQ4PTF/Hwu0JrH56FA3qucjKLeDsl35kZI/m/GtCf8AeJV3w+iLiUrIJcAoD24fRs1UoDve2sScx\nnaU7k04YIfWcTk25cUg7LurdsuSEbTUoKDSMeH0hLUKCCA5yEXMohWVPXFC8szsVWw+ncv+nv7In\nKYOr+keweEciSek5DGjXmA2xKTSo5+SGs9sxe10cCWnZ3HB2O5bsSOLgsUxuP7cj1w1qS6DLgcsp\nrNl3lNnr41i6MwmXQ3j0wu7ccV5HnA4hPjWb/y7ewyer9hPgFO67oAuX9W3N8l1JLNiSwNbDqbic\nQoDTQaDLQcNAFyGBLjbEHschwox7zqFDs4YcTsnihndXk5CazZg+rRjQvjGtG9XnHwu2E3MolQt6\nNOeWc9rTN6IxYWVLQSepTpRlfCEzN5+nvtzE7PUnXno+sH0Yr46PpLO7nLJydzL3f/oryRm290dw\noIseLUPo06YRvVuHussuhrwCQ1CAk74RjYqDcnxqNrd+8Evxj1F0Dm/INVFt6RIeTGj9AJwO+GbD\nYWaujSU9J5/QIBf/vmEAw8scTZS1Zt9R7vhwDWk5JcMo9G3bmJm/P6f4y2yM4ZJ/L2NzXCoTh3Xi\n6XE9Pc7r7UW7eHX+do/PAbQIDWRg+zAGtAtjRPfw4hKCMYaF2xP4cPk+2jdtwGMX9aBRfQ8nrU/T\nA9PXMWdDHJ2aNeS7B8733gPIbdbaWB75YgMAjeoHMLpnC37XqwXnd21Gw0AXB5Iz+b+vN7F0p+1r\nffvQDjw9ricBTgf7kjJ4ce5Wlu1MotAYjKHcUWCPliFMmtCPHi1DWbozkSdnbeLQ8SzqBzi5rG9r\nbhzS7oQjtYqs2XeUa95ZyVX92zB+YAT7j2by1Jeb+GziiTvi3YnpHDyayeCOTTyWH45n5vLD5nhi\nj2dxWd/WdGl+cqXA0/He0j288J292O3RC7tx/wVdT3uembn5PDd7M1+sjWVwhyY8Oa4HA9qFsTM+\njZfmbmXh9kS6tQjmtfF96du2MRk5+bw8bxsfr9pfbl5tGtfn0r6tmTCoLR08nLcoWudFJaSi10R1\nCEOwQ5zk5BeQnpNPek4+gS4nr1x91gmltPjUbJ6fs5nVe49y1B0nmocE8vxlvRnbp+VpH8UW0eB+\nEopq67HHsjjLHaybeihPJKblsDkuhS7Ng2nTuP5JrayUrDw+WrGPczo3ZWD7MI+vzcjJZ/GORCIj\nGtm+w1Vw8GgmMYdSaNkoiNaN69MsOLCkV4nbugPHmLk2lmcu6eU1myosNKzYnUx+YSGNG9QjNMhF\nUICTAKeDei4HoUEun22cp2JzXAoPf76e16/pW+WgOXfTYUKCXAzp1NRj5mqMYe6mIwS6HMUna71J\ny85jR3waWw+nkVdQyPWD253wWWbk5PPL3qMM7BBWXPKqKmMMj3yxgW82xBWPk9SpWUN+emR4jX7m\nJyM1O49zXvoJgBVPjip3Luh0HE7JomVoULnPYkd8Gu2bNiDQdeI2HXMohf3JmfbIM7+QLs2DGdAu\nrErnzFbsSiImLoXzu4bTo2XIKX3+xhgOHM1kR3w6gzs28Xmyo8FdKT+TnpPPqt3JrNidzMge4Zzf\n9fR/RP639PW6Q4jA5f3aVD6xOmUa3JVSqhaqanD3z7FllFJKVUiDu1JK1UIa3JVSqhaqUnAXkTEi\nsl1EdonIk16muVZEtojIZhH51LfNVEopdTIqvUJVRJzAW8DvgFhgjYjMMcZsKTVNV+ApYKgx5piI\nNK+uBiullKpcVTL3wcAuY8weY0wu8BlweZlp7gbeMsYcAzDGVPxLuUoppapVVYJ7G+Bgqfux7sdK\n6wZ0E5HlIrJKRMZ4mpGITBSRaBGJTkz8bX4BXCml6qKqBHdPl2iV7RzvAroCI4DrgfdEpNxlhMaY\nKcaYKGNMVHi4f12goZRS/qQqo0LGAqUHx44A4jxMs8oYkwfsFZHt2GC/xttM165dmyQi5QeBqJpm\nQNIpvtaf1cXlrovLDHVzueviMsPJL3f7yiepWnBfA3QVkY7AIWACcEOZab7GZuxTRaQZtkyzp6KZ\nGmNOOXUXkeiqXKFV29TF5a6Lywx1c7nr4jJD9S13pWUZY0w+cD/wPbAVmGGM2SwifxWRoh8D/B5I\nFpEtwELgMWNMsq8bq5RSqmqq9GMdxpi5wNwyjz1b6rYB/uT+U0opVcP89QrVKTXdgBpSF5e7Li4z\n1M3lrovLDNW03DU2KqRSSqnq46+Zu1JKqQpocFdKqVrI74J7VQYx83ci0lZEForIVvdAbA+6H28i\nIgtEZKf7f1hl8/JHIuIUkXUi8q37fkcRWe1e7s9F5PR+YfgMIyKNRWSmiGxzr/Nz6sK6FpGH3dt3\njIhMF5Gg2riuReQDEUkQkZhSj3lcv2K96Y5vG0VkwKm+r18F91KDmI0FegHXi0ivmm1VtcgHHjHG\n9ASGAPe5l/NJ4CdjTFfgJ/f92uhBbLfbIq8A/3Qv9zHgzhppVfX5FzDfGNMD6Itd9lq9rkWkDfAA\nEGWM6QM4sdfQ1MZ1PRUoOySLt/U7FnsBaFdgIjD5VN/Ur4I7VRvEzO8ZYw4bY351307DftnbYJf1\nf+7J/gdcUTMtrD4iEgFcDLznvi/ABcBM9yS1arlFJBQYBrwPYIzJNcYcpw6sa2xX7Poi4gIaAIep\nhevaGLMEOFrmYW/r93LgI2OtAhqLSKtTeV9/C+5VGcSsVhGRDkB/YDXQwhhzGOwOAKiNQytPAh4H\nCt33mwLH3RfTQe1b552AROBDdynqPRFpSC1f18aYQ8DrwAFsUE8B1lK713Vp3tavz2KcvwX3qgxi\nVmuISDAwC3jIGJNa0+2pbiJyCZBgjFlb+mEPk9amde4CBgCTjTH9gQxqWQnGE3eN+XKgI9AaaIgt\nSZRVm9Z1Vfhse/e34F6VQcxqBREJwAb2acaYL90Pxxcdorn/17Zx84cCl4nIPmzJ7QJsJt/YfegO\ntW+dxwKxxpjV7vszscG+tq/r0cBeY0yie8DBL4Fzqd3rujRv69dnMc7fgnvxIGbus+gTgDk13Caf\nc9eZ3we2GmPeKPXUHOBW9+1bgdm/dduqkzHmKWNMhDGmA3bd/myMuRE7XtF492S1armNMUeAgyLS\n3f3QKGALtXxdY8sxQ0SkgXt7L1ruWruuy/C2fucAt7h7zQwBUorKNyfNGONXf8A4YAewG/i/mm5P\nNS3jedhDsY3AevffOGz9+Sdgp/t/k5puazV+BiOAb923OwG/ALuAL4DAmm6fj5e1HxDtXt9fA2F1\nYV0DfwG2ATHAx0BgbVzXwHTseYU8bGZ+p7f1iy3LvOWOb5uwvYlO6X11+AGllKqF/K0so5RSqgo0\nuCulVC2kwV0ppWohDe5KKVULaXBXSqlaSIO7qrVEpEBE1pf689mVnyLSofQof0qdaar0G6pK+aks\nY0y/mm6EUjVBM3dV54jIPhF5RUR+cf91cT/eXkR+co+j/ZOItHM/3kJEvhKRDe6/c92zcorIu+4x\nyX8Qkfo1tlBKlaHBXdVm9cuUZa4r9VyqMWYw8B/s+DW4b39kjIkEpgFvuh9/E1hsjOmLHfdls/vx\nrsBbxpjewHHg6mpeHqWqTK9QVbWWiKQbY4I9PL4PuMAYs8c9QNsRY0xTEUkCWhlj8tyPHzbGNBOR\nRCDCGJNTah4dgAXG/tgCIvIEEGCMeaH6l0ypymnmruoq4+W2t2k8ySl1uwA9h6XOIBrcVV11Xan/\nKy5hOAAAAACWSURBVN23V2BHowS4EVjmvv0TcC8U/75r6G/VSKVOlWYaqjarLyLrS92fb4wp6g4Z\nKCKrsQnO9e7HHgA+EJHHsL+OdLv78QeBKSJyJzZDvxc7yp9SZyytuas6x11zjzLGJNV0W5SqLlqW\nUUqpWkgzd6WUqoU0c1dKqVpIg7tSStVCGtyVUqoW0uCulFK1kAZ3pZSqhf4fkC9kTktW8i4AAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a26f0a67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('Train History')\n",
    "# plt.ylabel(train)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
